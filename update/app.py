import streamlit as st
import pandas as pd
import pdfplumber
import collections
import re

# --- å…¨åŸŸå®šç¾©çš„é—œéµå­—åˆ—è¡¨ ---
credit_column_keywords = ["å­¸åˆ†", "å­¸åˆ†æ•¸", "å­¸åˆ†(GPA)", "å­¸ åˆ†", "Credits", "Credit", "å­¸åˆ†æ•¸(å­¸åˆ†)", "ç¸½å­¸åˆ†"]
subject_column_keywords = ["ç§‘ç›®åç¨±", "èª²ç¨‹åç¨±", "Course Name", "Subject Name", "ç§‘ç›®", "èª²ç¨‹"]
gpa_column_keywords = ["GPA", "æˆç¸¾", "Grade", "gpa(æ•¸å€¼)"]
year_column_keywords = ["å­¸å¹´", "year", "å­¸ å¹´", "å­¸å¹´åº¦"] # å¢åŠ äº†"å­¸å¹´åº¦"
semester_column_keywords = ["å­¸æœŸ", "semester", "å­¸ æœŸ"]
failing_grades = ["D", "D-", "E", "F", "X", "ä¸é€šé", "æœªé€šé", "ä¸åŠæ ¼"]

# å°‡æ‰€æœ‰æ¨™é ­é—œéµå­—æ‰å¹³åŒ–ç‚ºä¸€å€‹åˆ—è¡¨ï¼Œç”¨æ–¼æ›´å»£æ³›çš„æ¨™é ­è¡Œåˆ¤æ–·
# ä½¿ç”¨ set ä»¥æé«˜æŸ¥è©¢æ•ˆç‡ï¼Œä¸¦ç¢ºä¿å”¯ä¸€æ€§
all_header_keywords_flat_lower = set([
    k.lower() for k in 
    credit_column_keywords + 
    subject_column_keywords + 
    gpa_column_keywords + 
    year_column_keywords + 
    semester_column_keywords
])


# --- è¼”åŠ©å‡½æ•¸ ---
def normalize_text(cell_content):
    """
    æ¨™æº–åŒ–å¾ pdfplumber æå–çš„å–®å…ƒæ ¼å…§å®¹ã€‚
    è™•ç† None å€¼ã€pdfplumber çš„ Text ç‰©ä»¶å’Œæ™®é€šå­—ä¸²ã€‚
    å°‡å¤šå€‹ç©ºç™½å­—å…ƒï¼ˆåŒ…æ‹¬æ›è¡Œï¼‰æ›¿æ›ç‚ºå–®å€‹ç©ºæ ¼ï¼Œä¸¦å»é™¤å…©ç«¯ç©ºç™½ã€‚
    """
    if cell_content is None:
        return ""

    text = ""
    # æª¢æŸ¥æ˜¯å¦æ˜¯ pdfplumber çš„ Text ç‰©ä»¶ (å®ƒæœƒæœ‰ .text å±¬æ€§)
    if hasattr(cell_content, 'text'):
        text = str(cell_content.text)
    # å¦‚æœä¸æ˜¯ Text ç‰©ä»¶ï¼Œä½†æœ¬èº«æ˜¯å­—ä¸²
    elif isinstance(cell_content, str):
        text = cell_content
    # å…¶ä»–æƒ…æ³ï¼Œå˜—è©¦è½‰æ›ç‚ºå­—ä¸²
    else:
        text = str(cell_content)
    
    return re.sub(r'\s+', ' ', text).strip()

def make_unique_columns(columns_list):
    """
    å°‡åˆ—è¡¨ä¸­çš„æ¬„ä½åç¨±è½‰æ›ç‚ºå”¯ä¸€çš„åç¨±ï¼Œè™•ç†é‡è¤‡å’Œç©ºå­—ä¸²ã€‚
    å¦‚æœé‡åˆ°é‡è¤‡æˆ–ç©ºå­—ä¸²ï¼Œæœƒæ·»åŠ å¾Œç¶´ (ä¾‹å¦‚ 'Column_1', 'æ¬„ä½_2')ã€‚
    """
    seen = collections.defaultdict(int)
    unique_columns = []
    for col in columns_list:
        original_col_cleaned = normalize_text(col)
        
        # For empty or very short strings, use 'Column_X' format
        if not original_col_cleaned or len(original_col_cleaned) < 2: 
            name_base = "Column"
            current_idx = 1
            # Ensure generated Column_X is unique within unique_columns list
            while f"{name_base}_{current_idx}" in unique_columns:
                current_idx += 1
            name = f"{name_base}_{current_idx}"
        else:
            name = original_col_cleaned
        
        # Handle duplicate names by adding a suffix
        final_name = name
        counter = seen[name]
        while final_name in unique_columns:
            counter += 1
            final_name = f"{name}_{counter}" 
        
        unique_columns.append(final_name)
        seen[name] = counter # Update the max count for this base name

    return unique_columns

def parse_credit_and_gpa(text):
    """
    å¾å–®å…ƒæ ¼æ–‡æœ¬ä¸­è§£æå­¸åˆ†å’Œ GPAã€‚
    è€ƒæ…® "A 2" (GPAåœ¨å·¦ï¼Œå­¸åˆ†åœ¨å³) å’Œ "2 A" (å­¸åˆ†åœ¨å·¦ï¼ŒGPAåœ¨å³) çš„æƒ…æ³ã€‚
    è¿”å› (å­¸åˆ†, GPA)ã€‚å¦‚æœè§£æå¤±æ•—ï¼Œè¿”å› (0.0, "")ã€‚
    """
    text_clean = normalize_text(text)
    
    # é¦–å…ˆæª¢æŸ¥æ˜¯å¦æ˜¯ã€Œé€šéã€æˆ–ã€ŒæŠµå…ã€ç­‰é—œéµè©
    if text_clean.lower() in ["é€šé", "æŠµå…", "pass", "exempt"]:
        return 0.0, text_clean

    # å˜—è©¦åŒ¹é… "GPA å­¸åˆ†" æ¨¡å¼ (ä¾‹å¦‚ "A 2", "C- 3")
    match_gpa_credit = re.match(r'([A-Fa-f][+\-]?)\s*(\d+(\.\d+)?)', text_clean)
    if match_gpa_credit:
        gpa = match_gpa_credit.group(1).upper()
        try:
            credit = float(match_gpa_credit.group(2))
            if 0.0 <= credit <= 5.0: # å­¸åˆ†ç¯„åœèª¿æ•´ç‚ºå…è¨±0å­¸åˆ†
                return credit, gpa
        except ValueError:
            pass

    # å˜—è©¦åŒ¹é… "å­¸åˆ† GPA" æ¨¡å¼ (ä¾‹å¦‚ "2 A", "3 B-")
    match_credit_gpa = re.match(r'(\d+(\.\d+)?)\s*([A-Fa-f][+\-]?)', text_clean)
    if match_credit_gpa:
        try:
            credit = float(match_credit_gpa.group(1))
            gpa = match_credit_gpa.group(3).upper()
            if 0.0 <= credit <= 5.0: # å­¸åˆ†ç¯„åœèª¿æ•´ç‚ºå…è¨±0å­¸åˆ†
                return credit, gpa
        except ValueError:
            pass
            
    # å˜—è©¦åªåŒ¹é…å­¸åˆ† (ç´”æ•¸å­—)
    credit_only_match = re.search(r'(\d+(\.\d+)?)', text_clean)
    if credit_only_match:
        try:
            credit = float(credit_only_match.group(1))
            if 0.0 <= credit <= 5.0: # å­¸åˆ†ç¯„åœèª¿æ•´ç‚ºå…è¨±0å­¸åˆ†
                return credit, "" 
        except ValueError:
            pass

    # å˜—è©¦åªåŒ¹é… GPA (ç´”å­—æ¯)
    gpa_only_match = re.search(r'([A-Fa-f][+\-]?)', text_clean)
    if gpa_only_match:
        return 0.0, gpa_only_match.group(1).upper()

    return 0.0, ""

def is_grades_table(df):
    """
    åˆ¤æ–·ä¸€å€‹ DataFrame æ˜¯å¦ç‚ºæœ‰æ•ˆçš„æˆç¸¾å–®è¡¨æ ¼ã€‚
    é€éæª¢æŸ¥æ˜¯å¦å­˜åœ¨é æœŸçš„æ¬„ä½é—œéµå­—å’Œæ•¸æ“šå…§å®¹æ¨¡å¼ä¾†åˆ¤æ–·ã€‚
    """
    if df.empty or len(df.columns) < 3:
        return False

    # Normalize column names for keyword matching
    normalized_columns = {re.sub(r'\s+', '', col).lower(): col for col in df.columns.tolist()}
    
    # Check for direct header matches first
    has_credit_col_header = any(any(k in norm_col for k in credit_column_keywords) for norm_col in normalized_columns.keys())
    has_gpa_col_header = any(any(k in norm_col for k in gpa_column_keywords) for norm_col in normalized_columns.keys())
    has_subject_col_header = any(any(k in norm_col for k in subject_column_keywords) for norm_col in normalized_columns.keys())
    has_year_col_header = any(any(k in norm_col for k in year_column_keywords) for norm_col in normalized_columns.keys())
    has_semester_col_header = any(any(k in norm_col for k in semester_column_keywords) for norm_col in normalized_columns.keys())

    # If all header keywords are present, it's a strong indicator
    if has_subject_col_header and (has_credit_col_header or has_gpa_col_header) and has_year_col_header and has_semester_col_header:
        return True
    
    # If not all headers are present, check content patterns
    # The thresholds might need fine-tuning based on actual PDF quality
    # We are looking for at least 3 out of 4 essential types to consider it a grades table by content
    
    # Find columns by content patterns - we need at least one of each critical type
    found_year_by_content = False
    found_semester_by_content = False
    found_subject_by_content = False
    found_credit_or_gpa_by_content = False

    sample_rows_df = df.head(min(len(df), 20)) # Use first few rows for sampling

    for col_name in df.columns:
        sample_data = sample_rows_df[col_name].apply(normalize_text).tolist()
        total_sample_count = len(sample_data)
        if total_sample_count == 0:
            continue

        # Year-like column: contains 3 or 4 digit numbers (e.g., 111, 2024)
        if not found_year_by_content:
            year_like_cells = sum(1 for item_str in sample_data if (item_str.isdigit() and (len(item_str) == 3 or len(item_str) == 4)))
            if year_like_cells / total_sample_count >= 0.6: # High confidence
                found_year_by_content = True

        # Semester-like column: contains specific semester keywords
        if not found_semester_by_content:
            semester_like_cells = sum(1 for item_str in sample_data if item_str.lower() in ["ä¸Š", "ä¸‹", "æ˜¥", "å¤", "ç§‹", "å†¬", "1", "2", "3", "æ˜¥å­£", "å¤å­£", "ç§‹å­£", "å†¬å­£", "spring", "summer", "fall", "winter"])
            if semester_like_cells / total_sample_count >= 0.6: # High confidence
                found_semester_by_content = True

        # Subject-like column: contains mostly Chinese characters, not just digits/GPA
        if not found_subject_by_content:
            subject_like_cells = sum(1 for item_str in sample_data 
                                     if re.search(r'[\u4e00-\u9fa5]', item_str) and len(item_str) >= 2
                                     and not item_str.isdigit() and not re.match(r'^[A-Fa-f][+\-]?$', item_str)
                                     and not item_str.lower() in ["é€šé", "æŠµå…", "pass", "exempt", "æœªçŸ¥ç§‘ç›®"])
            if subject_like_cells / total_sample_count >= 0.4: # Moderate confidence
                found_subject_by_content = True

        # Credit/GPA-like column: contains numbers suitable for credits or grade letters
        if not found_credit_or_gpa_by_content:
            credit_gpa_like_cells = 0
            for item_str in sample_data:
                credit_val, gpa_val = parse_credit_and_gpa(item_str)
                if (0.0 <= credit_val <= 5.0) or \
                   (gpa_val and re.match(r'^[A-Fa-f][+\-]?$', gpa_val)) or \
                   (item_str.lower() in ["é€šé", "æŠµå…", "pass", "exempt"]):
                    credit_gpa_like_cells += 1
            if credit_gpa_like_cells / total_sample_count >= 0.4: # Moderate confidence
                found_credit_or_gpa_by_content = True
    
    # A table is considered a grades table if it has at least one of each crucial column type by content
    if found_year_by_content and found_semester_by_content and found_subject_by_content and found_credit_or_gpa_by_content:
        return True

    return False

def calculate_total_credits(df_list):
    """
    å¾æå–çš„ DataFrames åˆ—è¡¨ä¸­è¨ˆç®—ç¸½å­¸åˆ†ã€‚
    å°‹æ‰¾åŒ…å« 'å­¸åˆ†' æˆ– 'å­¸åˆ†(GPA)' é¡ä¼¼å­—æ¨£çš„æ¬„ä½é€²è¡ŒåŠ ç¸½ã€‚
    è¿”å›ç¸½å­¸åˆ†å’Œè¨ˆç®—å­¸åˆ†çš„ç§‘ç›®åˆ—è¡¨ï¼Œä»¥åŠä¸åŠæ ¼ç§‘ç›®åˆ—è¡¨ã€‚
    """
    total_credits = 0.0
    calculated_courses = [] 
    failed_courses = [] 

    for df_idx, df in enumerate(df_list):
        if df.empty or len(df.columns) < 3: # Skip empty or too small dataframes
            continue
        
        # Dictionary to store identified column names for each role
        identified_columns = {
            "year": None,
            "semester": None,
            "subject": None,
            "credit": None,
            "gpa": None
        }

        # Step 1: Prioritize identifying columns by their content patterns and relative positions
        potential_roles_scores = collections.defaultdict(list) # {role: [(col_name, score)]}

        sample_rows_df = df.head(min(len(df), 20)) # Sample first few rows for pattern detection

        for col_name in df.columns:
            sample_data = sample_rows_df[col_name].apply(normalize_text).tolist()
            total_sample_count = len(sample_data)
            if total_sample_count == 0:
                continue

            # Calculate scores for each role based on content
            # Year-like: 3 or 4 digits
            year_score = sum(1 for item_str in sample_data if (item_str.isdigit() and (len(item_str) == 3 or len(item_str) == 4))) / total_sample_count
            
            # Semester-like: specific keywords
            semester_score = sum(1 for item_str in sample_data if item_str.lower() in ["ä¸Š", "ä¸‹", "æ˜¥", "å¤", "ç§‹", "å†¬", "1", "2", "3", "æ˜¥å­£", "å¤å­£", "ç§‹å­£", "å†¬å­£", "spring", "summer", "fall", "winter"]) / total_sample_count
            
            # Credit/GPA-like: numbers for credits, or letter grades
            credit_gpa_score = 0
            for item_str in sample_data:
                credit_val, gpa_val = parse_credit_and_gpa(item_str)
                if (0.0 <= credit_val <= 5.0) or (gpa_val and re.match(r'^[A-Fa-f][+\-]?$', gpa_val)) or (item_str.lower() in ["é€šé", "æŠµå…", "pass", "exempt"]):
                    credit_gpa_score += 1
            credit_gpa_score /= total_sample_count

            # Subject-like: contains Chinese, not just numbers/GPA, longer than 1 char
            subject_score = sum(1 for item_str in sample_data 
                                 if re.search(r'[\u4e00-\u9fa5]', item_str) and len(item_str) >= 2
                                 and not item_str.isdigit() and not re.match(r'^[A-Fa-f][+\-]?$', item_str)
                                 and not item_str.lower() in ["é€šé", "æŠµå…", "pass", "exempt", "æœªçŸ¥ç§‘ç›®"]
                                 and not any(k in item_str.lower() for k in all_header_keywords_flat_lower) # Ensure it's not a header keyword
                                ) / total_sample_count

            if year_score >= 0.6: potential_roles_scores["year"].append((col_name, year_score))
            if semester_score >= 0.6: potential_roles_scores["semester"].append((col_name, semester_score))
            if credit_gpa_score >= 0.4: potential_roles_scores["credit_gpa"].append((col_name, credit_gpa_score)) # Group credit and gpa for now
            if subject_score >= 0.4: potential_roles_scores["subject"].append((col_name, subject_score))

        # Sort potentials by score (desc) then by column index (asc) to pick the best candidate
        for role in potential_roles_scores:
            potential_roles_scores[role].sort(key=lambda x: (-x[1], df.columns.get_loc(x[0]))) # Higher score first, then earlier column

        # Assign columns based on ranked potentials, prioritizing distinct roles
        
        # Year and Semester
        if potential_roles_scores["year"]:
            identified_columns["year"] = potential_roles_scores["year"][0][0]
        if potential_roles_scores["semester"]:
            identified_columns["semester"] = potential_roles_scores["semester"][0][0]
            # If year and semester are the same column, and it matches year pattern better, prioritize year
            if identified_columns["year"] == identified_columns["semester"] and identified_columns["year"] is not None:
                if potential_roles_scores["year"][0][1] > potential_roles_scores["semester"][0][1]:
                    identified_columns["semester"] = None # Let it be found by header if possible

        # Subject column is usually to the left of credit/GPA
        if potential_roles_scores["subject"]:
            identified_columns["subject"] = potential_roles_scores["subject"][0][0]

        # Credit/GPA - try to pick distinct ones
        credit_gpa_candidates = [cn for cn, _ in potential_roles_scores["credit_gpa"]]
        
        # Try to find a dedicated credit column first
        for c in credit_gpa_candidates:
            norm_col_name = re.sub(r'\s+', '', c).lower()
            # If header contains credit keywords OR content is mostly numbers fitting credit pattern
            if any(k in norm_col_name for k in credit_column_keywords) or \
               (sum(1 for item_str in sample_rows_df[c].apply(normalize_text).tolist() if re.match(r'^\d+(\.\d+)?$', item_str) and (0.0 <= float(item_str) <= 5.0)) / total_sample_count >= 0.5):
                identified_columns["credit"] = c
                break
        
        # Try to find a dedicated GPA column, distinct from credit
        for c in credit_gpa_candidates:
            if c == identified_columns["credit"]: continue # Don't re-use the credit column
            norm_col_name = re.sub(r'\s+', '', c).lower()
            # If header contains GPA keywords OR content is mostly letter grades
            if any(k in norm_col_name for k in gpa_column_keywords) or \
               (sum(1 for item_str in sample_rows_df[c].apply(normalize_text).tolist() if re.match(r'^[A-Fa-f][+\-]?$', item_str)) / total_sample_count >= 0.5):
                identified_columns["gpa"] = c
                break

        # Fallback for credit/GPA if still not distinct (take the best candidates)
        if identified_columns["credit"] is None and credit_gpa_candidates:
            identified_columns["credit"] = credit_gpa_candidates[0]
        if identified_columns["gpa"] is None and len(credit_gpa_candidates) > 1 and identified_columns["credit"] != credit_gpa_candidates[1]:
            identified_columns["gpa"] = credit_gpa_candidates[1]

        # Step 2: Use header keywords as a strong confirmation or fallback if content detection was weak
        normalized_df_columns = {re.sub(r'\s+', '', col_name).lower(): col_name for col_name in df.columns}

        for role, keywords in [
            ("year", year_column_keywords),
            ("semester", semester_column_keywords),
            ("subject", subject_column_keywords),
            ("credit", credit_column_keywords),
            ("gpa", gpa_column_keywords)
        ]:
            if identified_columns[role] is None:
                for k in keywords:
                    for norm_col_key, original_col_name in normalized_df_columns.items():
                        if k in norm_col_key:
                            identified_columns[role] = original_col_name
                            break
                    if identified_columns[role]: break
        
        found_year_column = identified_columns["year"]
        found_semester_column = identified_columns["semester"]
        found_subject_column = identified_columns["subject"]
        found_credit_column = identified_columns["credit"]
        found_gpa_column = identified_columns["gpa"]

        # Proceed only if essential columns are found (year, semester, subject, credit/gpa)
        if found_year_column and found_semester_column and found_subject_column and (found_credit_column or found_gpa_column):
            try:
                for row_idx, row in df.iterrows():
                    # åµéŒ¯è¼¸å‡ºï¼šé¡¯ç¤ºåŸå§‹è³‡æ–™åˆ—
                    st.info(f"--- è™•ç†è¡¨æ ¼ {df_idx + 1}, ç¬¬ {row_idx + 1} è¡Œ ---")
                    row_content = [normalize_text(str(cell)) for cell in row.tolist()]
                    st.info(f"åŸå§‹è³‡æ–™åˆ—å…§å®¹: {row_content}")
                    st.info(f"è­˜åˆ¥åˆ°çš„å­¸å¹´æ¬„ä½: '{found_year_column}' (å…§å®¹: {row.get(found_year_column, '') if found_year_column else 'N/A'}), å­¸æœŸæ¬„ä½: '{found_semester_column}' (å…§å®¹: {row.get(found_semester_column, '') if found_semester_column else 'N/A'})")
                    st.info(f"è­˜åˆ¥åˆ°çš„ç§‘ç›®æ¬„ä½: '{found_subject_column}' (å…§å®¹: {row.get(found_subject_column, '') if found_subject_column else 'N/A'}), å­¸åˆ†æ¬„ä½: '{found_credit_column}' (å…§å®¹: {row.get(found_credit_column, '') if found_credit_column else 'N/A'}), GPAæ¬„ä½: '{found_gpa_column}' (å…§å®¹: {row.get(found_gpa_column, '') if found_gpa_column else 'N/A'})")

                    # --- æ¨™é ­è¡Œå…§å®¹åˆ¤æ–·é‚è¼¯ï¼ˆå·²å­˜åœ¨ï¼Œå†æ¬¡ç¢ºèªç„¡èª¤ï¼‰ ---
                    is_header_row_content = False
                    header_keyword_matches = 0
                    for cell_val in row_content:
                        normalized_cell_val = normalize_text(cell_val).lower()
                        if normalized_cell_val in all_header_keywords_flat_lower:
                            header_keyword_matches += 1
                    
                    if header_keyword_matches >= 3: 
                        is_header_row_content = True

                    if all(cell == "" for cell in row_content) or \
                       is_header_row_content or \
                       any("é«”è‚²å®¤" in cell or "æœ¬è¡¨åƒ…ä¾›æŸ¥è©¢" in cell or "å­¸è™Ÿ" in cell or "å‹ä½œ" in cell for cell in row_content):
                        st.info("è©²è¡Œè¢«åˆ¤æ–·ç‚ºç©ºè¡Œã€æ¨™é ­è¡Œæˆ–è¡Œæ”¿æ€§æ–‡å­—ï¼Œå·²è·³éã€‚")
                        continue

                    extracted_credit = 0.0
                    extracted_gpa = ""

                    # --- å­¸åˆ†å’Œ GPA å„ªå…ˆå¾å„è‡ªæ¬„ä½æå–ï¼Œå†è€ƒæ…®åˆä½µæ¬„ä½ ---
                    if found_credit_column in row and pd.notna(row[found_credit_column]):
                        extracted_credit, _ = parse_credit_and_gpa(row[found_credit_column])
                        # å†æ¬¡æª¢æŸ¥å­¸åˆ†æœ‰æ•ˆæ€§
                        if not (0.0 <= extracted_credit <= 5.0):
                            extracted_credit = 0.0 # ç„¡æ•ˆå­¸åˆ†ï¼Œè¨­ç‚º0

                    if found_gpa_column and found_gpa_column in row and pd.notna(row[found_gpa_column]):
                        _, extracted_gpa_from_gpa_col = parse_credit_and_gpa(row[found_gpa_column])
                        if extracted_gpa_from_gpa_col:
                            extracted_gpa = extracted_gpa_from_gpa_col.upper()
                    
                    # --- è™•ç†å­¸åˆ†å’Œ GPA å¯èƒ½åœ¨åŒä¸€å€‹å„²å­˜æ ¼çš„æƒ…æ³ (ä¾‹å¦‚ "3 A") ---
                    if extracted_credit == 0.0 and not extracted_gpa: # å¦‚æœéƒ½æ²’æå–åˆ°ï¼Œå†å˜—è©¦åˆä½µè§£æ
                        # æª¢æŸ¥å¯èƒ½çš„åˆä½µæ¬„ä½ (ä¾‹å¦‚ç§‘ç›®åç¨±å³å´ï¼Œå­¸åˆ†/GPAå·¦å´)
                        # é€™é‚Šéœ€è¦æ›´å…·é«”çš„åˆ¤æ–·ï¼Œæš«æ™‚å…ˆæª¢æŸ¥ç§‘ç›®åç¨±æ¬„ä½ï¼Œå› ç‚ºç§‘ç›®åç¨±æ¬„ä½æœ‰æ™‚æœƒé€£å¸¶å­¸åˆ†æˆ–GPA
                        if found_subject_column in row and pd.notna(row[found_subject_column]):
                            combined_text = normalize_text(row[found_subject_column])
                            temp_credit, temp_gpa = parse_credit_and_gpa(combined_text)
                            if temp_credit > 0:
                                extracted_credit = temp_credit
                            if temp_gpa and not extracted_gpa:
                                extracted_gpa = temp_gpa.upper()
                        # è€ƒæ…®é¸èª²ä»£è™Ÿæ¬„ä½å³é‚Šæ˜¯å¦æœ‰å­¸åˆ†/GPA (é‡å°æŸäº›ç‰¹å®šæ ¼å¼)
                        current_col_idx_subject = df.columns.get_loc(found_subject_column)
                        if current_col_idx_subject > 0:
                            prev_col_name = df.columns[current_col_idx_subject - 1] # å¯èƒ½æ˜¯é¸èª²ä»£è™Ÿ
                            if prev_col_name in row and pd.notna(row[prev_col_name]):
                                combined_text_prev = normalize_text(row[prev_col_name])
                                temp_credit, temp_gpa = parse_credit_and_gpa(combined_text_prev)
                                if temp_credit > 0 and extracted_credit == 0.0:
                                    extracted_credit = temp_credit
                                if temp_gpa and not extracted_gpa:
                                    extracted_gpa = temp_gpa.upper()


                    is_failing_grade = False
                    if extracted_gpa:
                        gpa_clean = re.sub(r'[+\-]', '', extracted_gpa).upper()
                        if gpa_clean in failing_grades or (gpa_clean.isdigit() and float(gpa_clean) < 60):
                            is_failing_grade = True
                        elif gpa_clean.replace('.', '', 1).isdigit() and float(gpa_clean) < 60:
                            is_failing_grade = True
                    
                    is_passed_or_exempt_grade = False
                    grade_text_for_pass_check = ""
                    if found_gpa_column and found_gpa_column in row and pd.notna(row[found_gpa_column]):
                        grade_text_for_pass_check += normalize_text(row[found_gpa_column]).lower()
                    if found_credit_column and found_credit_column in row and pd.notna(row[found_credit_column]):
                        grade_text_for_pass_check += " " + normalize_text(row[found_credit_column]).lower()

                    if "é€šé" in grade_text_for_pass_check or "æŠµå…" in grade_text_for_pass_check or "pass" in grade_text_for_pass_check or "exempt" in grade_text_for_pass_check:
                        is_passed_or_exempt_grade = True
                        if extracted_credit == 0.0: # å¦‚æœæ˜¯é€šé/æŠµå…ä½†å­¸åˆ†æ²’æŠ“åˆ°ï¼Œå¯èƒ½æ˜¯0å­¸åˆ†èª²ç¨‹
                            extracted_credit = 0.0 # ç¢ºä¿ç‚º0

                    course_name = "" 
                    if found_subject_column in row and pd.notna(row[found_subject_column]):
                        temp_name = normalize_text(row[found_subject_column])
                        # èª²ç¨‹åç¨±çš„éæ¿¾æ¢ä»¶ï¼Œç¢ºä¿ä¸æ˜¯æ•¸å­—ã€GPA æˆ–è¡Œæ”¿æ–‡å­—
                        if len(temp_name) >= 1 and re.search(r'[\u4e00-\u9fa5]', temp_name) and \
                           not temp_name.isdigit() and not re.match(r'^[A-Fa-f][+\-]?$', temp_name) and \
                           not temp_name.lower() in ["é€šé", "æŠµå…", "pass", "exempt", "æœªçŸ¥ç§‘ç›®"] and \
                           not any(kw in temp_name for kw in all_header_keywords_flat_lower) and \
                           not any(re.search(pattern, temp_name) for pattern in ["å­¸è™Ÿ", "æœ¬è¡¨", "è¨»èª²çµ„", "å¹´ç´š", "ç­ç´š", "ç³»åˆ¥", "ç•¢æ¥­é–€æª»", "é«”è‚²å¸¸è­˜"]): # å¢åŠ æ›´å¤šè¡Œæ”¿æ–‡å­—éæ¿¾
                            course_name = temp_name
                        else: # å¦‚æœç§‘ç›®æ¬„ä½å…§å®¹ä¸ç¬¦åˆèª²ç¨‹åç¨±ï¼Œæª¢æŸ¥å·¦å³å…©æ¬„
                            current_col_idx = df.columns.get_loc(found_subject_column)
                            for offset in [-1, 1]: # æª¢æŸ¥å·¦é‚Šå’Œå³é‚Šçš„æ¬„ä½
                                check_col_idx = current_col_idx + offset
                                if 0 <= check_col_idx < len(df.columns):
                                    neighbor_col_name = df.columns[check_col_idx]
                                    if neighbor_col_name in row and pd.notna(row[neighbor_col_name]):
                                        temp_name_neighbor = normalize_text(row[neighbor_col_name])
                                        if len(temp_name_neighbor) >= 1 and re.search(r'[\u4e00-\u9fa5]', temp_name_neighbor) and \
                                           not temp_name_neighbor.isdigit() and not re.match(r'^[A-Fa-f][+\-]?$', temp_name_neighbor) and \
                                           not any(kw in temp_name_neighbor for kw in all_header_keywords_flat_lower) and \
                                           not any(re.search(pattern, temp_name_neighbor) for pattern in ["å­¸è™Ÿ", "æœ¬è¡¨", "è¨»èª²çµ„", "å¹´ç´š", "ç­ç´š", "ç³»åˆ¥", "ç•¢æ¥­é–€æª»", "é«”è‚²å¸¸è­˜"]):
                                            course_name = temp_name_neighbor
                                            break # æ‰¾åˆ°å°±è·³å‡º

                    if not course_name: # æœ€çµ‚ç§‘ç›®åç¨±ä»ç‚ºç©ºï¼Œè¨­ç‚º"æœªçŸ¥ç§‘ç›®"
                        course_name = "æœªçŸ¥ç§‘ç›®"


                    # --- æå–å­¸å¹´å’Œå­¸æœŸ (åŠ å¼·å°ã€Œ111\nä¸Šã€é€™é¡æ ¼å¼çš„è™•ç†) ---
                    acad_year = ""
                    semester = ""
                    
                    # å„ªå…ˆå¾è­˜åˆ¥åˆ°çš„å­¸å¹´å’Œå­¸æœŸæ¬„ä½ç²å–
                    if found_year_column in row and pd.notna(row[found_year_column]):
                        temp_year_sem_combined = normalize_text(row[found_year_column])
                        year_match = re.search(r'(\d{3,4})', temp_year_sem_combined)
                        if year_match:
                            acad_year = year_match.group(1)
                        
                        # åŒä¸€æ¬„ä½ä¹Ÿå¯èƒ½åŒ…å«å­¸æœŸ
                        sem_match = re.search(r'(ä¸Š|ä¸‹|æ˜¥|å¤|ç§‹|å†¬|1|2|3|æ˜¥å­£|å¤å­£|ç§‹å­£|å†¬å­£|spring|summer|fall|winter)', temp_year_sem_combined, re.IGNORECASE)
                        if sem_match:
                            semester = sem_match.group(1)

                    if not semester and found_semester_column in row and pd.notna(row[found_semester_column]):
                        temp_sem = normalize_text(row[found_semester_column])
                        sem_match = re.search(r'(ä¸Š|ä¸‹|æ˜¥|å¤|ç§‹|å†¬|1|2|3|æ˜¥å­£|å¤å­£|ç§‹å­£|å†¬å­£|spring|summer|fall|winter)', temp_sem, re.IGNORECASE)
                        if sem_match:
                            semester = sem_match.group(1)

                    # Fallback for year/semester if not found in dedicated columns (e.g., if they are in the first few generic columns)
                    # æª¢æŸ¥å‰å¹¾å€‹é€šç”¨æ¬„ä½
                    for col_idx in range(min(len(df.columns), 3)): # æª¢æŸ¥å‰3å€‹æ¬„ä½
                        col_name = df.columns[col_idx]
                        if col_name in row and pd.notna(row[col_name]):
                            col_content = normalize_text(row[col_name])
                            if not acad_year:
                                year_match = re.search(r'(\d{3,4})', col_content)
                                if year_match:
                                    acad_year = year_match.group(1)
                            if not semester:
                                sem_match = re.search(r'(ä¸Š|ä¸‹|æ˜¥|å¤|ç§‹|å†¬|1|2|3|æ˜¥å­£|å¤å­£|ç§‹å­£|å†¬å­£|spring|summer|fall|winter)', col_content, re.IGNORECASE)
                                if sem_match:
                                    semester = sem_match.group(1)
                            if acad_year and semester: # å…©å€‹éƒ½æ‰¾åˆ°äº†å°±åœæ­¢
                                break
                    
                    # åµéŒ¯è¼¸å‡ºï¼šé¡¯ç¤ºè§£æçµæœ
                    st.info(f"è§£æçµæœ: ç§‘ç›®åç¨±='{course_name}', å­¸åˆ†='{extracted_credit}', GPA='{extracted_gpa}', å­¸å¹´='{acad_year}', å­¸æœŸ='{semester}', æ˜¯å¦ä¸åŠæ ¼='{is_failing_grade}'")

                    # å¦‚æœç§‘ç›®åç¨±ç‚º"æœªçŸ¥ç§‘ç›®"ä¸”å­¸åˆ†æˆ–GPAç‚ºç©ºï¼Œå‰‡è·³é (ç¢ºä¿åªè™•ç†æœ‰æ•ˆæ•¸æ“š)
                    if course_name == "æœªçŸ¥ç§‘ç›®" and extracted_credit == 0.0 and not extracted_gpa and not is_passed_or_exempt_grade:
                        st.info("è©²è¡Œæ²’æœ‰è­˜åˆ¥åˆ°ç§‘ç›®åç¨±ã€æœ‰æ•ˆå­¸åˆ†æˆ–æˆç¸¾ï¼Œå·²è·³éã€‚")
                        continue

                    if is_failing_grade:
                        failed_courses.append({
                            "å­¸å¹´åº¦": acad_year,
                            "å­¸æœŸ": semester,
                            "ç§‘ç›®åç¨±": course_name, 
                            "å­¸åˆ†": extracted_credit, 
                            "GPA": extracted_gpa, 
                            "ä¾†æºè¡¨æ ¼": df_idx + 1
                        })
                    elif extracted_credit > 0 or is_passed_or_exempt_grade:
                        if extracted_credit > 0: 
                            total_credits += extracted_credit
                        calculated_courses.append({
                            "å­¸å¹´åº¦": acad_year,
                            "å­¸æœŸ": semester,
                            "ç§‘ç›®åç¨±": course_name, 
                            "å­¸åˆ†": extracted_credit, 
                            "GPA": extracted_gpa, 
                            "ä¾†æºè¡¨æ ¼": df_idx + 1
                        })
                
            except Exception as e:
                st.warning(f"è¡¨æ ¼ {df_idx + 1} çš„å­¸åˆ†è¨ˆç®—æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e}`ã€‚è©²è¡¨æ ¼çš„å­¸åˆ†å¯èƒ½ç„¡æ³•è¨ˆå…¥ç¸½æ•¸ã€‚è«‹æª¢æŸ¥å­¸åˆ†å’ŒGPAæ¬„ä½æ•¸æ“šæ˜¯å¦æ­£ç¢ºã€‚")
        else:
            st.info(f"é é¢ {df_idx + 1} çš„è¡¨æ ¼æœªèƒ½è­˜åˆ¥ç‚ºæˆç¸¾å–®è¡¨æ ¼ (ç¼ºå°‘å¿…è¦çš„ å­¸å¹´/å­¸æœŸ/ç§‘ç›®åç¨±/å­¸åˆ†/GPA æ¬„ä½)ã€‚å·²åµæ¸¬åˆ°çš„æ¬„ä½: å­¸å¹´='{found_year_column}', å­¸æœŸ='{found_semester_column}', ç§‘ç›®åç¨±='{found_subject_column}', å­¸åˆ†='{found_credit_column}', GPA='{found_gpa_column}'")
            
    return total_credits, calculated_courses, failed_courses

def process_pdf_file(uploaded_file):
    """
    ä½¿ç”¨ pdfplumber è™•ç†ä¸Šå‚³çš„ PDF æª”æ¡ˆï¼Œæå–è¡¨æ ¼ã€‚
    æ­¤å‡½æ•¸å…§éƒ¨å°‡æ¸›å°‘ Streamlit çš„ç›´æ¥è¼¸å‡ºï¼Œåªè¿”å›æå–çš„æ•¸æ“šã€‚
    """
    all_grades_data = []

    try:
        with pdfplumber.open(uploaded_file) as pdf:
            for page_num, page in enumerate(pdf.pages):
                current_page = page 

                # èª¿æ•´ç­–ç•¥ï¼šä½¿ç”¨ 'text' ç­–ç•¥ï¼Œä¸¦é€²ä¸€æ­¥èª¿æ•´ text_tolerance, snap_tolerance, join_tolerance
                # é€™äº›å€¼æ˜¯ç‚ºäº†æ›´å¥½åœ°è™•ç†æ‰‹æ©Ÿæƒææˆ–ç”Ÿæˆçš„ä¸è¦å‰‡ PDF è¡¨æ ¼
                table_settings = {
                    "vertical_strategy": "text", 
                    "horizontal_strategy": "text", 
                    "snap_tolerance": 15,  # ç‚ºäº†æ›´å¥½çš„æ‰‹æ©Ÿæª”æ¡ˆåµæ¸¬ï¼Œç¨å¾®å¢å¤§ï¼Œå…è¨±æ–‡å­—èˆ‡ç·šæ¢é–“éš”æ›´å¤§
                    "join_tolerance": 15,  # ç‚ºäº†æ›´å¥½çš„æ‰‹æ©Ÿæª”æ¡ˆåµæ¸¬ï¼Œç¨å¾®å¢å¤§ï¼Œå…è¨±ç·šæ¢æ–·è£‚æ›´é•·
                    "edge_min_length": 3, 
                    "text_tolerance": 8,  # ç‚ºäº†æ›´å¥½çš„æ‰‹æ©Ÿæª”æ¡ˆåµæ¸¬ï¼Œç¨å¾®å¢å¤§ï¼Œå…è¨±æ–‡å­—å°é½Šåå·®æ›´å¤§
                    "min_words_vertical": 1, 
                    "min_words_horizontal": 1, 
                }
                
                try:
                    tables = current_page.extract_tables(table_settings)

                    if not tables:
                        st.info(f"é é¢ **{page_num + 1}** æœªåµæ¸¬åˆ°è¡¨æ ¼ã€‚é€™å¯èƒ½æ˜¯ç”±æ–¼ PDF æ ¼å¼è¤‡é›œæˆ–è¡¨æ ¼æå–è¨­å®šä¸é©ç”¨ã€‚")
                        continue

                    for table_idx, table in enumerate(tables):
                        processed_table = []
                        for row in table:
                            normalized_row = [normalize_text(cell) for cell in row]
                            # Filter out rows that are entirely empty after normalization
                            if any(cell.strip() != "" for cell in normalized_row):
                                processed_table.append(normalized_row)
                        
                        if not processed_table:
                            st.info(f"é é¢ {page_num + 1} çš„è¡¨æ ¼ **{table_idx + 1}** æå–å¾Œç‚ºç©ºæˆ–å…¨ç‚ºç©ºç™½è¡Œã€‚")
                            continue
                        
                        df_table_to_add = None

                        # --- Attempt 1: Assume first row is header if it strongly looks like one ---
                        if len(processed_table) > 1:
                            potential_header_row = processed_table[0]
                            # Create a temporary DataFrame to test if this looks like a grades table header
                            # Use generic columns for the temp df, then check if its first row (which is the actual potential header) contains keywords
                            max_cols_temp = max(len(cell_list) for cell_list in processed_table) # Get max columns from all processed rows
                            temp_generic_columns = make_unique_columns([f"TempCol_{i+1}" for i in range(max_cols_temp)])
                            
                            # Ensure potential_header_row has enough elements to match temp_generic_columns
                            padded_header_row = potential_header_row + [''] * (max_cols_temp - len(potential_header_row))

                            temp_df_for_header_check = pd.DataFrame([padded_header_row], columns=temp_generic_columns)
                            
                            # Check if temp_df_for_header_check's *content* (i.e., the first row) contains header keywords
                            # Normalizing columns for keyword search
                            temp_normalized_header_content = {re.sub(r'\s+', '', cell).lower(): cell for cell in padded_header_row}
                            
                            has_credit_kw = any(any(k in cell for k in credit_column_keywords) for cell in temp_normalized_header_content.keys())
                            has_gpa_kw = any(any(k in cell for k in gpa_column_keywords) for cell in temp_normalized_header_content.keys()) 
                            has_subject_kw = any(any(k in cell for k in subject_column_keywords) for cell in temp_normalized_header_content.keys())
                            has_year_kw = any(any(k in cell for k in year_column_keywords) for cell in temp_normalized_header_content.keys())
                            has_semester_kw = any(any(k in cell for k in semester_column_keywords) for cell in temp_normalized_header_content.keys())

                            if has_subject_kw and (has_credit_kw or has_gpa_kw) and has_year_kw and has_semester_kw: # Strong header match
                                temp_unique_columns = make_unique_columns(potential_header_row)
                                temp_data_rows = processed_table[1:]

                                num_cols_for_df = len(temp_unique_columns)
                                cleaned_temp_data_rows = []
                                for row in temp_data_rows:
                                    if len(row) > num_cols_for_df:
                                        cleaned_temp_data_rows.append(row[:num_cols_for_df])
                                    elif len(row) < num_cols_for_df: 
                                        cleaned_temp_data_rows.append(row + [''] * (num_cols_for_df - len(row)))
                                    else:
                                        cleaned_temp_data_rows.append(row)

                                if cleaned_temp_data_rows:
                                    try:
                                        df_table_with_assumed_header = pd.DataFrame(cleaned_temp_data_rows, columns=temp_unique_columns)
                                        if is_grades_table(df_table_with_assumed_header):
                                            df_table_to_add = df_table_with_assumed_header
                                            st.success(f"é é¢ {page_num + 1} çš„è¡¨æ ¼ {table_idx + 1} å·²è­˜åˆ¥ç‚ºæˆç¸¾å–®è¡¨æ ¼ (å¸¶æœ‰åµæ¸¬åˆ°çš„æ¨™é ­)ã€‚")
                                    except Exception as e_df_temp:
                                        st.warning(f"é é¢ {page_num + 1} è¡¨æ ¼ {table_idx + 1} å˜—è©¦ç”¨ç¬¬ä¸€è¡Œä½œæ¨™é ­è½‰æ›ç‚º DataFrame æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_df_temp}`ã€‚å°‡å˜—è©¦å°‡æ‰€æœ‰è¡Œä½œç‚ºæ•¸æ“šã€‚")
                                else:
                                    st.info(f"é é¢ {page_num + 1} çš„è¡¨æ ¼ {table_idx + 1} ç¬¬ä¸€è¡Œè¢«è­˜åˆ¥ç‚ºæ¨™é ­ä½†ç„¡æ•¸æ“šè¡Œï¼Œå°‡å˜—è©¦å°‡æ‰€æœ‰è¡Œä½œç‚ºæ•¸æ“šã€‚")
                        
                        # --- Attempt 2: Treat all rows as data (generic columns) if Attempt 1 failed or was not applicable ---
                        if df_table_to_add is None:
                            max_cols = max(len(row) for row in processed_table)
                            generic_columns = make_unique_columns([f"Column_{i+1}" for i in range(max_cols)])

                            cleaned_all_rows_data = []
                            for row in processed_table:
                                if len(row) > max_cols:
                                    cleaned_all_rows_data.append(row[:max_cols])
                                elif len(row) < max_cols:
                                    cleaned_all_rows_data.append(row + [''] * (max_cols - len(row)))
                                else:
                                    cleaned_all_rows_data.append(row)
                            
                            if cleaned_all_rows_data:
                                try:
                                    df_table_all_data = pd.DataFrame(cleaned_all_rows_data, columns=generic_columns)
                                    if is_grades_table(df_table_all_data):
                                        df_table_to_add = df_table_all_data
                                        st.success(f"é é¢ {page_num + 1} çš„è¡¨æ ¼ {table_idx + 1} å·²è­˜åˆ¥ç‚ºæˆç¸¾å–®è¡¨æ ¼ (æ‰€æœ‰è¡Œçš†ç‚ºæ•¸æ“šï¼Œä½¿ç”¨é€šç”¨æ¨™é ­)ã€‚")
                                    else:
                                        st.info(f"é é¢ {page_num + 1} çš„è¡¨æ ¼ {table_idx + 1} æœªèƒ½è­˜åˆ¥ç‚ºæˆç¸¾å–®è¡¨æ ¼ï¼Œå·²è·³éã€‚")
                                except Exception as e_df_all:
                                    st.error(f"é é¢ {page_num + 1} è¡¨æ ¼ {table_idx + 1} å˜—è©¦ç”¨æ‰€æœ‰è¡Œä½œæ•¸æ“šè½‰æ›ç‚º DataFrame æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_df_all}`")
                            else:
                                st.info(f"é é¢ {page_num + 1} çš„è¡¨æ ¼ **{table_idx + 1}** æ²’æœ‰æœ‰æ•ˆæ•¸æ“šè¡Œã€‚")

                        if df_table_to_add is not None:
                            all_grades_data.append(df_table_to_add)

                except Exception as e_table:
                    st.error(f"é é¢ **{page_num + 1}** è™•ç†è¡¨æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_table}`")
                    st.warning("é€™å¯èƒ½æ˜¯ç”±æ–¼ PDF æ ¼å¼è¤‡é›œæˆ–è¡¨æ ¼æå–è¨­å®šä¸é©ç”¨ã€‚è«‹æª¢æŸ¥ PDF çµæ§‹ã€‚")

    except pdfplumber.PDFSyntaxError as e_pdf_syntax:
        st.error(f"è™•ç† PDF èªæ³•æ™‚ç™¼ç”ŸéŒ¯èª¤: `{e_pdf_syntax}`ã€‚æª”æ¡ˆå¯èƒ½å·²æå£æˆ–æ ¼å¼ä¸æ­£ç¢ºã€‚")
    except Exception as e:
        st.error(f"è™•ç† PDF æª”æ¡ˆæ™‚ç™¼ç”Ÿä¸€èˆ¬éŒ¯èª¤: `{e}`")
        st.error("è«‹ç¢ºèªæ‚¨çš„ PDF æ ¼å¼æ˜¯å¦ç‚ºæ¸…æ™°çš„è¡¨æ ¼ã€‚è‹¥å•é¡ŒæŒçºŒï¼Œå¯èƒ½æ˜¯ PDF çµæ§‹è¼ƒç‚ºè¤‡é›œï¼Œéœ€è¦èª¿æ•´ `pdfplumber` çš„è¡¨æ ¼æå–è¨­å®šã€‚")

    return all_grades_data

# --- Streamlit æ‡‰ç”¨ä¸»é«” ---
def main():
    st.set_page_config(page_title="PDF æˆç¸¾å–®å­¸åˆ†è¨ˆç®—å·¥å…·", layout="wide")
    st.title("ğŸ“„ PDF æˆç¸¾å–®å­¸åˆ†è¨ˆç®—å·¥å…·")

    st.write("è«‹ä¸Šå‚³æ‚¨çš„ PDF æˆç¸¾å–®æª”æ¡ˆï¼Œå·¥å…·å°‡å˜—è©¦æå–å…¶ä¸­çš„è¡¨æ ¼æ•¸æ“šä¸¦è¨ˆç®—ç¸½å­¸åˆ†ã€‚")
    st.write("æ‚¨ä¹Ÿå¯ä»¥è¼¸å…¥ç›®æ¨™å­¸åˆ†ï¼ŒæŸ¥çœ‹é‚„å·®å¤šå°‘å­¸åˆ†ã€‚")

    uploaded_file = st.file_uploader("é¸æ“‡ä¸€å€‹ PDF æª”æ¡ˆ", type="pdf")

    if uploaded_file is not None:
        st.success(f"å·²ä¸Šå‚³æª”æ¡ˆ: **{uploaded_file.name}**")
        with st.spinner("æ­£åœ¨è™•ç† PDFï¼Œè«‹ç¨å€™..."):
            extracted_dfs = process_pdf_file(uploaded_file)

        if extracted_dfs:
            st.markdown("---")
            st.markdown("## âš™ï¸ åµéŒ¯è³‡è¨Š (Debug Info)")
            st.info("ä»¥ä¸‹æ˜¯ç¨‹å¼ç¢¼è™•ç†æ¯è¡Œæ•¸æ“šçš„è©³ç´°éç¨‹ï¼Œå¹«åŠ©æ‚¨ç†è§£å­¸åˆ†è¨ˆç®—å’Œèª²ç¨‹è­˜åˆ¥çš„ç‹€æ³ã€‚")
            st.info("æ‚¨ä¸Šå‚³çš„åŸå§‹è¡¨æ ¼å…§å®¹å°‡æœƒé¡¯ç¤ºï¼Œä»¥åŠç¨‹å¼ç¢¼å¦‚ä½•è§£æå„å€‹æ¬„ä½ã€‚")
            st.info("å¦‚æœæ‚¨ç™¼ç¾æœ‰èª¤ï¼Œè«‹æ ¹æ“šé€™äº›è³‡è¨Šå‘ŠçŸ¥æˆ‘å…·é«”æ˜¯å“ªå€‹è¡¨æ ¼çš„å“ªä¸€è¡Œã€å“ªå€‹æ¬„ä½æœ‰å•é¡Œã€‚")
            st.info("--- åµéŒ¯è¨Šæ¯çµæŸ ---") # æ·»åŠ é€™è¡Œä»¥æ˜ç¢ºå€åˆ†åµéŒ¯è¼¸å‡º

            total_credits, calculated_courses, failed_courses = calculate_total_credits(extracted_dfs)

            st.markdown("---")
            st.markdown("## âœ… æŸ¥è©¢çµæœ") 
            st.markdown(f"ç›®å‰ç¸½å­¸åˆ†: <span style='color:green; font-size: 24px;'>**{total_credits:.2f}**</span>", unsafe_allow_html=True)
            
            target_credits = st.number_input("è¼¸å…¥æ‚¨çš„ç›®æ¨™å­¸åˆ† (ä¾‹å¦‚ï¼š128)", min_value=0.0, value=128.0, step=1.0, 
                                            help="æ‚¨å¯ä»¥è¨­å®šä¸€å€‹ç•¢æ¥­å­¸åˆ†ç›®æ¨™ï¼Œå·¥å…·æœƒå¹«æ‚¨è¨ˆç®—é‚„å·®å¤šå°‘å­¸åˆ†ã€‚")
            
            credit_difference = target_credits - total_credits
            if credit_difference > 0:
                st.write(f"è·é›¢ç•¢æ¥­æ‰€éœ€å­¸åˆ† (å…±{target_credits:.0f}å­¸åˆ†) é‚„å·® **{credit_difference:.2f}**")
            elif credit_difference < 0:
                st.write(f"å·²è¶…è¶Šç•¢æ¥­å­¸åˆ† (å…±{target_credits:.0f}å­¸åˆ†) **{abs(credit_difference):.2f}**")
            else:
                st.write(f"å·²é”åˆ°ç•¢æ¥­æ‰€éœ€å­¸åˆ† (å…±{target_credits:.0f}å­¸åˆ†) **0.00**")


            st.markdown("---")
            st.markdown("### ğŸ“š é€šéçš„èª²ç¨‹åˆ—è¡¨") 
            if calculated_courses:
                courses_df = pd.DataFrame(calculated_courses)
                display_cols = ['å­¸å¹´åº¦', 'å­¸æœŸ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA']
                final_display_cols = [col for col in display_cols if col in courses_df.columns]
                
                st.dataframe(courses_df[final_display_cols], height=300, use_container_width=True) 
            else:
                st.info("æ²’æœ‰æ‰¾åˆ°å¯ä»¥è¨ˆç®—å­¸åˆ†çš„ç§‘ç›®ã€‚")

            if failed_courses:
                st.markdown("---")
                st.markdown("### âš ï¸ ä¸åŠæ ¼çš„èª²ç¨‹åˆ—è¡¨")
                failed_df = pd.DataFrame(failed_courses)
                display_failed_cols = ['å­¸å¹´åº¦', 'å­¸æœŸ', 'ç§‘ç›®åç¨±', 'å­¸åˆ†', 'GPA', 'ä¾†æºè¡¨æ ¼']
                final_display_failed_cols = [col for col in display_failed_cols if col in failed_df.columns]
                st.dataframe(failed_df[final_display_failed_cols], height=200, use_container_width=True)
                st.info("é€™äº›ç§‘ç›®å› æˆç¸¾ä¸åŠæ ¼ ('D', 'E', 'F' ç­‰) è€Œæœªè¨ˆå…¥ç¸½å­¸åˆ†ã€‚")

            if calculated_courses or failed_courses:
                if calculated_courses:
                    csv_data_passed = pd.DataFrame(calculated_courses).to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ä¸‹è¼‰é€šéçš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                        data=csv_data_passed,
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_calculated_courses.csv",
                        mime="text/csv",
                        key="download_passed_btn"
                    )
                if failed_courses:
                    csv_data_failed = pd.DataFrame(failed_courses).to_csv(index=False, encoding='utf-8-sig')
                    st.download_button(
                        label="ä¸‹è¼‰ä¸åŠæ ¼çš„ç§‘ç›®åˆ—è¡¨ç‚º CSV",
                        data=csv_data_failed,
                        file_name=f"{uploaded_file.name.replace('.pdf', '')}_failed_courses.csv",
                        mime="text/csv",
                        key="download_failed_btn"
                    )
            
        else:
            st.warning("æœªå¾ PDF ä¸­æå–åˆ°ä»»ä½•è¡¨æ ¼æ•¸æ“šã€‚è«‹æª¢æŸ¥ PDF å…§å®¹æˆ–å˜—è©¦èª¿æ•´ `pdfplumber` çš„è¡¨æ ¼æå–è¨­å®šã€‚")
    else:
        st.info("è«‹ä¸Šå‚³ PDF æª”æ¡ˆä»¥é–‹å§‹è™•ç†ã€‚")

if __name__ == "__main__":
    main()
